# -*- coding: utf-8 -*-
"""Adversarial_DQN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gZdfcfakPRHkkYbjCv8v1GzLVR8iFyQB

# Adversarial Simple Spread - DQN

Agent observations: `[self_is_adversary, self_vel, self_pos, landmark_rel_positions, other_agent_is_adversary_rel_positions]`
 - `self_is_adversary = (1, )`: 0 / 1 flag
 - `self_vel = (2, )`
 - `self_pos = (2, )`
 - `landmark_rel_positions = (2 * n_landmarks, )`
 - `other_agent_is_adversary_rel_positions = ((1 + 2) * (n_agents + n_adversaries - 1), )`: 0 / 1 flag  for if that other agent is an adversary + relative position for the other agent times the number of other agents

Agent action space: `[no_action, move_left, move_right, move_down, move_up] = (0-4)`
"""

# Commented out IPython magic to ensure Python compatibility.
# %load_ext autoreload
# %autoreload 2

!pip install gymnasium
!pip install pettingzoo

from collections import namedtuple
import imageio
from IPython.display import display, Image
import matplotlib
import matplotlib.pyplot as plt
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F

import simple_spread_adversarial_comm_symmetric

# Set up matplotlib:
is_ipython = 'inline' in matplotlib.get_backend()
if is_ipython:
    from IPython import display

plt.ion()

# Detect if GPU is available:
DEVICE = torch.device(
    "mps" if torch.backends.mps.is_available()
    else (
        "cuda" if torch.cuda.is_available()
        else "cpu"
    )
)

# Global params:
# Horizon is not truly infinite so lower this to approximate a finite horizon
# (GAMMA^25 < 0.01, whereas 0.99^25 = ~0.78)
GAMMA = 0.8

"""## Set Up Environment"""

# Set up environment

env = simple_spread_adversarial_comm_symmetric.parallel_env(n_agents = 2, n_adversaries=2, n_landmarks = 2, comm_mode=5)

observations, _ = env.reset()
observation_dim = observations["agent_0"].shape[0]
num_actions = env.action_space("agent_0").n

print("num_agents = ", env.num_agents, env.agents)
print(f"{num_actions=}")
print(f"{observation_dim=}")

observations

"""## Helpers"""

class DQN(nn.Module):
    def __init__(
            self,
            input_dim=observation_dim,
            layers=[],
            output_dim=num_actions,
            activation_function="ReLU",
            loss_function="huber_loss",
            **kwargs
        ):
        super().__init__()

        # Validate Inputs:
        assert len(layers) > 0

        if type(activation_function) == str:
            assert(activation_function in nn.__dict__), (
                "activation_function must be one of the activation functions " +
                "available in the torch.nn module, e.g. 'ReLU'"
            )
            activation_function = nn.__dict__[activation_function]

        if type(loss_function) == str:
            assert(loss_function in F.__dict__), (
                "loss_function must be one of the loss functions " +
                "available in the torch.nn.functional module, e.g. 'cross_entropy'"
            )
            loss_function = F.__dict__[loss_function]

        # ------------------------------
        # Actually Initialize the Model:
        # ------------------------------
        self.layers = nn.ModuleList()

        # Hidden MLP Layers:
        for i, layer_size in enumerate(layers):
            if i == 0:
                layer_in = input_dim
            else:
                layer_in = layers[i - 1]

            layer = nn.Sequential(
                nn.Linear(layer_in, layer_size),
                activation_function()
            )
            self.layers.append(layer)

        # Final Fully Connected Output Layer:
        self.layers.append(nn.Linear(layers[-1], output_dim))

        # Loss:
        self.loss_function = loss_function

    def copy_weights(self, other_model):
        self.load_state_dict(other_model.state_dict())

    def forward(self, X, targets=None):
        for layer in self.layers:
            X = layer(X)

        # Final layer's output is the logits:
        logits = X

        if targets is not None:
            loss = self.loss_function(logits, targets)
            return logits, loss
        else:
            return logits

# Models objects for holding all the models for an agent:
Models = namedtuple("Models", ["main", "target"])

def _initialize_models(model_size):
    # Model used for selecting actions (principal)
    model = DQN(layers=[model_size] * 2).to(DEVICE)

    # Then create the target model. This will periodically be copied from the principal network
    target_model = DQN(layers=[model_size] * 2).to(DEVICE)

    # Make sure models are initialized identically:
    target_model.copy_weights(model)

    return Models(model, target_model)

def initialize_models(env, model_size):
    env.reset()  # Reset env to make sure agent list isn't empty
    return {agent: _initialize_models(model_size) for agent in env.agents}

def select_actions(env, observations, models, epsilon):
    # Epsilon greedy
    if np.random.random() < epsilon:
        # Take random actions:
        actions = {agent: env.action_space(agent).sample() for agent in env.agents}
    else:
        # Predict action Q-values
        # From environment observations
        actions = {}
        for agent, model in models.items():
            model.main.eval()
            actions[agent] = model.main(
                # Observation (state) as tensor
                torch.tensor(observations[agent]).to(DEVICE)
            ).argmax(dim=-1).item()

    return actions

class ReplayBuffer():
    def __init__(
            self,
            max_size,
            state_dim=observation_dim,
            action_dim=1,
            reward_dim=1,
        ):
        self.history = []
        self.max_size = max_size
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.reward_dim = reward_dim

    @property
    def size(self):
        return len(self.history)

    def states(self, i):
        return self.history[i, :self.state_dim]

    def actions(self, i):
        return self.history[i, self.state_dim:(self.state_dim + self.action_dim)]

    def rewards(self, i):
        return self.history[
            i,
            (self.state_dim + self.action_dim)
            :(self.state_dim + self.action_dim + self.reward_dim)
        ]

    def state_nexts(self, i):
        return self.history[i, -self.state_dim:]

    def store(self, state, action, reward, state_next):
        if self.action_dim == 1: action = [action]
        if self.reward_dim == 1: reward = [reward]

        SARS = np.concatenate(
            [state, action, reward, state_next],
            dtype="float32"
        ).reshape(1, -1)

        if self.size == 0:
            self.history = SARS
        else:
            self.history = np.concatenate([self.history, SARS], axis=0)

        # Don't let the memory grow beyond the limit
        if self.size > self.max_size:
            self.history = self.history[1:]

    def sample(self, n):
        #  Sample a set of batch_size memories from the history
        sample_ids = np.random.choice(
            np.arange(self.size), size=n, replace=False
        )

        states = torch.tensor(self.states(sample_ids)).to(DEVICE)
        actions = torch.tensor(self.actions(sample_ids)).to(DEVICE)
        rewards = torch.tensor(self.rewards(sample_ids)).to(DEVICE)
        state_nexts = torch.tensor(self.state_nexts(sample_ids)).to(DEVICE)

        return states, actions, rewards, state_nexts

def train_one_step(models, memories, optimizers, batch_size=128):
    losses = {}

    for agent, model in models.items():
        #  Sample a set of batch_size memories from the history
        states, actions, rewards, state_nexts = memories[agent].sample(batch_size)

        # Create for the sample states the targets (r+gamma * max Q(...) )
        model.target.eval()
        Q_next_state = model.target(state_nexts)
        Q_targets = rewards + GAMMA * Q_next_state.max(axis=-1).values.view(-1, 1)

        # Q values to train:
        model.main.train()
        Q_values = model.main(states)
        # Pluck out the relevant actions only
        Q_of_actions = Q_values.gather(1, actions.to(int))

        # Calculate loss and perform optimization step:
        loss = model.main.loss_function(Q_of_actions, Q_targets)
        optimizers[agent].zero_grad()
        loss.backward()
        optimizers[agent].step()

        losses[agent] = loss.item()

    return losses

# Build validation dateset of held out states for use in tracking avg. max Q as
# a training validation / debugging metric:
def generate_validation_data(env=env, n=100):
    env.reset()  # Reset env to make sure agent list isn't empty
    validation_data = {agent: [] for agent in env.agents}

    for episode in range(n):
        observations, _ = env.reset()

        while env.agents:
            for agent in env.agents:
                validation_data[agent].append(observations[agent])

            # Take random actions:
            actions = {agent: env.action_space(agent).sample() for agent in env.agents}
            # Follow selected actions:
            observations, _, _, _, _ = env.step(actions)

    env.reset()  # Reset env to make sure agent list isn't empty
    validation_data = {
        agent: torch.tensor(np.array(validation_data[agent])).to(DEVICE)
        for agent in env.agents
    }
    return validation_data

def calc_avg_max_Q(models, validation_data):
    avg_max_Qs = {}
    for agent, model in models.items():
        model.main.eval()
        avg_max_Qs[agent] =\
            model.main(validation_data[agent]).max(axis=-1).values.mean().item()

    return avg_max_Qs

"""## Validation Set Up"""

validation_data = generate_validation_data(env=env, n=100)

"""## Training Loop"""

# ==============================================================================
# KEY PARAMS:
model_size = 128
batch_size = 128
learning_rate = 5e-4
max_episodes = 5000
max_memory = 10000  # Replay memory size
# how often to train your model - this allows you to speed up learning
# by not performing in every iteration learning. See also refernece paper
# you can set this value to other values like 1 as well to learn every time
update_after_actions = 4
target_update_every = 1000  # How often to update the target network
epsilon = 0.1
# ==============================================================================

# Initialize models:
models = initialize_models(env, model_size)
optimizers = {
    agent: optim.Adam(model.main.parameters(), lr=learning_rate)
    for agent, model in models.items()
}

# Our Experience Replay memory (state, action, reward, next state)
env.reset()  # Reset env to make sure agent list isn't empty
memories = {agent: ReplayBuffer(max_memory) for agent in env.agents}
episode_reward_history = {agent: [] for agent in env.agents}
loss_history = {agent: [] for agent in env.agents}
avg_max_Q_history = {agent: [] for agent in env.agents}

# Tracking values
running_rewards = {agent: 0 for agent in env.agents}
episode_count = 0
timestep_count = 0

# Training Loop
for episode in range(max_episodes):
    observations, _ = env.reset()
    episode_rewards = {agent: 0 for agent in env.agents}

    while env.agents:
        timestep_count += 1
        actions = select_actions(env, observations, models, epsilon)

        # Follow selected actions
        observations_next, rewards, _, _, _ = env.step(actions)

        # Save action results
        for agent in rewards.keys():
            episode_rewards[agent] += rewards[agent]

            # Save action/states and other information in replay buffer
            memories[agent].store(
                observations[agent],
                actions[agent],
                rewards[agent],
                observations_next[agent]
            )

        observations = observations_next

        # Update every Xth frame to speed up (optional)
        # and if you have sufficient history
        if timestep_count % update_after_actions == 0 and timestep_count > batch_size:
            losses = train_one_step(models, memories, optimizers, batch_size)
            for agent, loss in losses.items():
                loss_history[agent].append(loss)

        if timestep_count % target_update_every == 0:
            # Update the the target network with new weights
            for model in models.values():
                model.target.copy_weights(model.main)

    # Record tracking statistics:
    for agent, episode_reward in episode_rewards.items():
        episode_reward_history[agent].append(episode_reward)

    for agent, avg_max_Q in calc_avg_max_Q(models, validation_data).items():
        avg_max_Q_history[agent].append(avg_max_Q)

    episode_count += 1

    # Logging:
    if episode_count % 50 == 0:
        # Log details
        # average reward of last 100:
        running_rewards = {
            agent: np.mean(episode_reward_history[agent][-100:])
            for agent in running_rewards.keys()
        }
        template = "running rewards: {} at episode {}, timestep {}, epsilon {}"
        print(template.format(running_rewards, episode_count, timestep_count, epsilon))

# Training Plots (for Debugging)
env.reset()  # Reset env to make sure agent list isn't empty

color_chooser = lambda agent: "darkred" if agent.startswith("adversary") else "darkblue"


fig, ax = plt.subplots(1, len(env.agents), figsize=(14.5, 5))
for i, agent in enumerate(env.agents):
    # Raw Reward
    ax[i].set_title("Reward / Episode" + f" ({agent})")
    ax[i].set_xlabel("Episode")
    ax[i].set_ylabel("Total Reward")
    ax[i].plot(
        np.arange(len(episode_reward_history[agent])),
        episode_reward_history[agent],
        color=color_chooser(agent)
    )
plt.show()

# Rolling Average Reward:
def moving_average(x, w):
    # Source: https://stackoverflow.com/a/54628145
    return np.convolve(x, np.ones(w), 'valid') / w

fig, ax = plt.subplots(1, len(env.agents), figsize=(14.5, 5))
for i, agent in enumerate(env.agents):
    rolling_means = moving_average(episode_reward_history[agent], 100)

    # Rolling Average
    ax[i].set_title("Rolling Mean Reward" + f" ({agent})")
    ax[i].set_xlabel("Episode")
    ax[i].set_ylabel("Average Total Reward (Last 100 Episodes)")
    ax[i].plot(
        np.arange(len(rolling_means)) + 100,
        rolling_means,
        color=color_chooser(agent)
    )
plt.show()

fig, ax = plt.subplots(1, len(env.agents), figsize=(14.5, 5))
for i, agent in enumerate(env.agents):
    # Loss Trend
    ax[i].set_title("Loss History" + f" ({agent})")
    ax[i].set_xlabel("Training Step")
    ax[i].set_ylabel("Loss")
    ax[i].plot(
        np.arange(len(loss_history[agent])),
        loss_history[agent],
        color=color_chooser(agent)
    )
plt.show()

fig, ax = plt.subplots(1, len(env.agents), figsize=(14.5, 5))
for i, agent in enumerate(env.agents):
    # Avg. Max Q Trend
    ax[i].set_title("Avg. Max Q Value History" + f" ({agent})")
    ax[i].set_xlabel("Episode")
    ax[i].set_ylabel("Avg. Max Q Value")
    ax[i].plot(
        np.arange(len(avg_max_Q_history[agent])),
        avg_max_Q_history[agent],
        color=color_chooser(agent)
    )
plt.show()

"""## Final Evaluation"""

def eval_greedy_model(env, models):
    env.reset()  # Reset env to make sure agent list isn't empty

    # Tracking values
    final_eval_rewards = {agent: [] for agent in env.agents}

    for episode in range(100):
        observations, _ = env.reset()
        episode_rewards = {agent: 0 for agent in env.agents}

        while env.agents:
            actions = select_actions(env, observations, models, epsilon=0)

            # Follow selected actions
            observations_next, rewards, _, _, _ = env.step(actions)

            # Save action results
            for agent in rewards.keys():
                episode_rewards[agent] += rewards[agent]

            observations = observations_next

        # Record tracking statistics:
        for agent, episode_reward in episode_rewards.items():
            final_eval_rewards[agent].append(episode_reward)


    avg_greedy_reward = {
        agent: np.mean(final_eval_reward)
        for agent, final_eval_reward in final_eval_rewards.items()
    }

    return avg_greedy_reward, final_eval_rewards

avg_greedy_reward, final_eval_rewards = eval_greedy_model(env, models)

print(avg_greedy_reward)

env.reset()  # Reset env to make sure agent list isn't empty

for agent in env.agents:
    # Raw Reward
    plt.title("Reward / Episode")
    plt.xlabel("Episode")
    plt.ylabel("Total Reward")
    plt.plot(np.arange(len(final_eval_rewards[agent])), final_eval_rewards[agent])
    plt.show()

# Nice multi-image visulation code for IPython
# Source: https://mindtrove.info/
from IPython.display import HTML, Image

def _src_from_data(data):
    """Base64 encodes image bytes for inclusion in an HTML img element"""
    img_obj = Image(data=data)
    for bundle in img_obj._repr_mimebundle_():
        for mimetype, b64value in bundle.items():
            if mimetype.startswith('image/'):
                return f'data:{mimetype};base64,{b64value}'

def gallery(images, row_height='auto'):
    """Shows a set of images in a gallery that flexes with the width of the notebook.

    Parameters
    ----------
    images: list of str or bytes
        URLs or bytes of images to display

    row_height: str
        CSS height value to assign to all images. Set to 'auto' by default to show images
        with their native dimensions. Set to a value like '250px' to make all rows
        in the gallery equal height.
    """
    figures = []
    for image in images:
        if isinstance(image, bytes):
            src = _src_from_data(image)
            caption = ''
        else:
            src = image
            caption = f'<figcaption style="font-size: 0.6em">{image}</figcaption>'
        figures.append(f'''
            <figure style="margin: 5px !important;">
              <img src="{src}" style="height: {row_height}">
              {caption}
            </figure>
        ''')
    return HTML(data=f'''
        <div style="display: flex; flex-flow: row wrap; text-align: center;">
        {''.join(figures)}
        </div>
    ''')

rendered_env = simple_spread_adversarial_comm_symmetric.parallel_env(n_adversaries = 2, n_agents = 2, n_landmarks = 2, comm_mode=5, render_mode="rgb_array")
frames = []

observations, _ = rendered_env.reset()
frames.append(rendered_env.render())

while rendered_env.agents:
    # this is where you would insert your policy
    actions = select_actions(rendered_env, observations, models, epsilon=0)

    observations, _, _, _, _ = rendered_env.step(actions)
    frames.append(rendered_env.render())

rendered_env.close()

imageio.mimwrite(
    "rendered_episode.gif",
    frames,
    loop=0  # Infinite loop gif
)